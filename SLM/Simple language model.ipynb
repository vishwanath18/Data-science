{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73b97bd4-07ab-4c65-8e9e-727a9c04a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk import bigrams, FreqDist, ConditionalFreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13252ac6-eb03-4606-9867-0d1da8f8e5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "070bae95-195e-42cf-ae0a-50507878d0a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(filepath)\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m text_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     15\u001b[0m     filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_data_dir, filename)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_hidden(filepath):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'data'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "\n",
    "## Keep your training documents in a folder named 'data'\n",
    "input_data_dir = \"data\"\n",
    "\n",
    "# String of punctuation without the full stop\n",
    "punctuation = string.punctuation.replace('.', '')  # Retain the full stop\n",
    "\n",
    "def is_hidden(filepath):\n",
    "    return os.path.basename(filepath).startswith('.')\n",
    "\n",
    "text_data=\"\"\n",
    "for filename in os.listdir(input_data_dir):\n",
    "    filepath = os.path.join(input_data_dir, filename)\n",
    "    if not is_hidden(filepath):\n",
    "        with open(filepath) as infile:\n",
    "            for line in infile:\n",
    "                if line.strip():  # Check if line is not just whitespace\n",
    "                    # Remove all punctuation except full stops\n",
    "                    for char in punctuation:\n",
    "                        line = line.replace(char, '')\n",
    "                    text_data += line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b76179f-ba7d-4b32-b922-84b137b51c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9602e493-3d0a-4d2c-909e-f75db249d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text into words\n",
    "# Lowercasing for consistency\n",
    "words = nltk.word_tokenize(text_data.lower())\n",
    "\n",
    "# Generate bigrams\n",
    "bi_grams = list(bigrams(words))\n",
    "\n",
    "# Calculate frequency distribution for each bigram\n",
    "bi_gram_freq_dist = FreqDist(bi_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1c34679-7a3f-4853-a936-162229849714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "# Print the first five elements of the dictionary\n",
    "first_five_items = list(islice(bi_gram_freq_dist.items(), 5))\n",
    "for item in first_five_items:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cb46e37-f483-4f28-b1d9-b3c100241e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute conditional frequency distribution of bigrams\n",
    "bi_gram_freq = ConditionalFreqDist(bi_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8662cf5-df8f-46df-917a-769211389b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_gram_freq['natural']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d5149a7-75db-48ec-b50d-e2c035818ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "topk=3\n",
    "# Create a dictionary to hold the top topk bigrams for each first word\n",
    "top_bigrams_per_first_word = {}\n",
    "\n",
    "# Iterate over the bigram frequency distribution\n",
    "for (first_word, second_word), freq in bi_gram_freq_dist.items():\n",
    "    # Initialize an empty heap for the first_word if it doesn't exist\n",
    "    if first_word not in top_bigrams_per_first_word:\n",
    "        top_bigrams_per_first_word[first_word] = []\n",
    "\n",
    "    # Add to the heap and maintain top topk\n",
    "    heapq.heappush(top_bigrams_per_first_word[first_word],\n",
    "                   (freq, second_word))\n",
    "    if len(top_bigrams_per_first_word[first_word]) > topk:\n",
    "        heapq.heappop(top_bigrams_per_first_word[first_word])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c642849-aa7f-477b-95bf-9374d6807ea6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'natural'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtop_bigrams_per_first_word\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnatural\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'natural'"
     ]
    }
   ],
   "source": [
    "top_bigrams_per_first_word['natural']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27541cba-c63b-436b-b554-e50d446c9b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the heap to a simple list for each first word\n",
    "for first_word in top_bigrams_per_first_word:\n",
    "    sorted_bigrams = sorted(\n",
    "        top_bigrams_per_first_word[first_word], reverse=True)\n",
    "    top_bigrams_list = []\n",
    "    for freq, second_word in sorted_bigrams:\n",
    "        top_bigrams_list.append(second_word)\n",
    "    top_bigrams_per_first_word[first_word] = top_bigrams_list\n",
    "\n",
    "# Use these filtered bigrams to create a ConditionalFreqDist\n",
    "filtered_bi_grams = []\n",
    "for first_word in top_bigrams_per_first_word:\n",
    "    for second_word in top_bigrams_per_first_word[first_word]:\n",
    "        filtered_bi_grams.append((first_word, second_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67835927-c7fb-4c6a-bb08-f3135c2eb3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(word, num_words):\n",
    "    word =word.lower()\n",
    "    for _ in range(num_words):\n",
    "        print(word, end=' ')\n",
    "        next_words = [item for item, freq in bi_gram_freq[word].items()]\n",
    "        if len(next_words) > 0:\n",
    "            # Randomly choose a next word\n",
    "            word = random.choice(next_words)\n",
    "        else:\n",
    "            break  # Break if the word has no following words\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27bb0de0-7fce-40ec-940d-97f2137f84da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asia \n"
     ]
    }
   ],
   "source": [
    "generate_sentence('Asia', 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
